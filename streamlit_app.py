# -*- coding: utf-8 -*-
"""streamlit_app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17851i43oWXKjZd9QJwxs4wnr5ejrh-ZR
"""

from google.colab import files
uploaded = files.upload()  # Upload metadata.csv first

# ✅ Step 2: Import Libraries
import os
import pandas as pd
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.utils import to_categorical

# ✅ Step 3: Load Metadata (Assumes metadata.csv was uploaded)
metadata_path = "metadata.csv"
df = pd.read_csv(metadata_path)
print("Total entries:", len(df))
df.head()

# ✅ Step 4: Upload all images manually
print("Now upload all image files (multiple select allowed)...")
image_files = files.upload()  # Upload all the .jpeg images manually

# ✅ Step 5: Preprocess Images
IMG_SIZE = 128

def load_image_from_uploads(img_name):
    if img_name in image_files:
        try:
            img = Image.open(img_name).convert('RGB')
            img = img.resize((IMG_SIZE, IMG_SIZE))
            return np.array(img)
        except Exception as e:
            print(f"❌ Error loading {img_name}: {e}")
            return None
    else:
        print(f"❌ Skipped (not uploaded): {img_name}")
        return None

images = []
labels = []

for idx, row in df.iterrows():
    img = load_image_from_uploads(row['imagefile'])
    if img is not None:
        images.append(img)
        labels.append(row['target'])

X = np.array(images) / 255.0  # Normalize
y = np.array(labels)

print("Image data shape:", X.shape)
print("Label shape:", y.shape)

# ✅ Step 6: Exploratory Data Analysis (EDA)
# Histogram of class distribution
plt.figure(figsize=(6,4))
df['target'].value_counts().plot(kind='bar', color=['orange', 'green'])
plt.title('Class Distribution')
plt.xlabel('Class (0 = No Weapon, 1 = Weapon)')
plt.ylabel('Count')
plt.grid(True)
plt.show()

# Scatter plot of sample image brightness vs. index
avg_brightness = [img.mean() for img in X]
plt.figure(figsize=(8,4))
plt.scatter(range(len(avg_brightness)), avg_brightness, c=y, cmap='coolwarm', alpha=0.7)
plt.title("Image Brightness vs. Sample Index")
plt.xlabel("Sample Index")
plt.ylabel("Average Brightness")
plt.colorbar(label='Target')
plt.show()

# Correlation heatmap of metadata (if available)
if 'brightness' in df.columns:
    plt.figure(figsize=(6,5))
    sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
    plt.title("Correlation Heatmap")
    plt.show()

# ✅ Step 7: Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ✅ Step 8: Build CNN Model
model = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)),
    MaxPooling2D(2,2),
    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D(2,2),
    Flatten(),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.summary()

# ✅ Step 9: Train Model
history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))

# ✅ Step 10: Evaluate Model
loss, acc = model.evaluate(X_test, y_test)
print("Test Accuracy:", acc)

# Classification Report
y_pred = (model.predict(X_test) > 0.5).astype("int32")
print(classification_report(y_test, y_pred))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()